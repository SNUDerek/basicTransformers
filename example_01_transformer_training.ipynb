{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee299a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b16260",
   "metadata": {},
   "source": [
    "## training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dc3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 8000\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 64\n",
    "GRAD_CLIP = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91041dd0",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "this is being trained for English > Korean translation on the `korean-english-news-v1` dataset from https://github.com/jungyeul/korean-parallel-corpora \n",
    "\n",
    "no preprocessing is done except to read in the data and write the (en, kr) pairs to tsv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57366ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/translation_news/en-ko-train.tsv\"\n",
    "valid_file = \"data/translation_news/en-ko-dev.tsv\"\n",
    "train_tokenizer = \"data/translation_news/src_tokenizer.pkl\"\n",
    "valid_tokenizer = \"data/translation_news/tgt_tokenizer.pkl\"\n",
    "checkpoint_file = \"data/translation_news/checkpoint.pt\"\n",
    "sample_sentences = [\n",
    "    \"After keeping the world's most powerful supercomputer to themselves for a year, government researchers showed off the $110 million wonder and said it might help save the world from nuclear war.\",\n",
    "    \"Most of the people involved in the discussion agree that there is a legitimate area in which the government needs to retain the right to intercept communications.\",\n",
    "    \"Several Texan transmission companies announced Monday they were forming a consortium to invest in the $5 billion cost of building new power lines to take advantage of the state's vast wind power.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a54e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for training with the alternate 'bible' dataset.\n",
    "# # here, the verse notes are removed, and every tenth item starting from the second (index = 1) is set to validation\n",
    "# train_file = \"data/translation_bible/train_pairs.tsv\"\n",
    "# valid_file = \"data/translation_bible/valid_pairs.tsv\"\n",
    "# train_tokenizer = \"data/translation_bible/src_tokenizer.pkl\"\n",
    "# valid_tokenizer = \"data/translation_bible/tgt_tokenizer.pkl\"\n",
    "# checkpoint_file = \"data/translation_bible/checkpoint.pt\"\n",
    "# sample_sentences = [\n",
    "#     \"The weapons we fight with are not the weapons of the world. On the contrary, they have divine power to demolish strongholds.\",\n",
    "#     \"Make it your ambition to lead a quiet life, to mind your own business and to work with your hands, just as we told you,\",\n",
    "#     \"It had a great, high wall with twelve gates, and with twelve angels at the gates. On the gates were written the names of the twelve tribes of Israel.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10879940",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = SimpleTranslationDataset(train_file, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf695d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(src_tokenizer, open(train_tokenizer, \"wb\"))\n",
    "pickle.dump(tgt_tokenizer, open(valid_tokenizer, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SimpleTranslationDataset(valid_file, \n",
    "                                         src_tokenizer=src_tokenizer, \n",
    "                                         tgt_tokenizer=tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"valid samples: {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87d254",
   "metadata": {},
   "source": [
    "## create model, etc\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- a small amount of dropout is added to the QK.T (`attn_dropout`) and to the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT\n",
    "- the pre-layernorm configuration is used\n",
    "- AdamW optimizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=256,\n",
    "     d_in=512, \n",
    "     d_attn=64, \n",
    "     d_ffnn=1024, \n",
    "     attn_heads=8, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.05, \n",
    "     ffnn_dropout=0.05,\n",
    "     pos_encoding=\"sinusoidal\",\n",
    "     shared_vocab=False,\n",
    "     attn_mask_val=-1e08, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09791aa",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "we'll use the `OneCycleLR` to roughly approximate the warmup and annealing by 'warming up' for 1 epoch and then decaying for 49 epochs (until 50th epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(mytransformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.0001, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=len(train_dataloader)*50, pct_start=1./50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5fddd",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5597aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    print(\"starting epoch {} of {}\".format(epoch+1, MAX_EPOCHS))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader, desc=\"minibatch\", total=len(train_dataloader)) as b:\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(mytransformer.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            windowed_losses.append(loss.item())\n",
    "            windowed_losses = windowed_losses[-16:]\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            b.set_postfix(loss=\"{:.3f}\".format(np.mean(windowed_losses)), global_step=global_step)\n",
    "            b.update(1)\n",
    "            \n",
    "    # end of epoch loss\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f} (end of epoch)\".format(\n",
    "        tme, epoch+1, global_step, loss.item(), np.mean(windowed_losses)\n",
    "    ))\n",
    "        \n",
    "    # evaluate\n",
    "    eval_losses = []\n",
    "    time.sleep(1)\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    mytransformer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(valid_dataloader):\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "            eval_losses.append(loss.item())\n",
    "            \n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # infer some results\n",
    "    print(\"\\n sample greedy outputs:\\n\")\n",
    "    with torch.no_grad():\n",
    "        for sample in sample_sentences:\n",
    "            x, x_len = src_tokenizer.transform(sample, as_array=True, bos=True, eos=True, max_len=MAX_SEQ_LEN)\n",
    "            x = torch.from_numpy(x).long().to(\"cuda\")\n",
    "            x_len = torch.from_numpy(x_len).long().to(\"cuda\")\n",
    "            y_hat = mytransformer.infer_one_greedy(x, x_len, bos=2, eos=3)\n",
    "            y_hat = tgt_tokenizer.inverse_transform([y_hat], as_tokens=False)[0]\n",
    "            print(\"\\tsrc: {}\".format(sample))\n",
    "            print(\"\\tprd: {}\\n\".format(y_hat))\n",
    "    \n",
    "    \n",
    "    # save\n",
    "    torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': mytransformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'windowed_losses': windowed_losses,\n",
    "            'avg_loss': np.mean(windowed_losses),\n",
    "            'eval_loss': np.mean(eval_losses),\n",
    "            'training_config': mytransformer.config,\n",
    "            'batch_size': BATCH_SIZE\n",
    "            }, checkpoint_file)\n",
    "    \n",
    "    print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "    \n",
    "    \n",
    "    mytransformer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b55b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
