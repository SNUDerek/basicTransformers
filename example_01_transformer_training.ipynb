{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368b313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0d114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8299067",
   "metadata": {},
   "source": [
    "## training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761e979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 8000\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9911b6f",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "this is being trained for English > Korean translation on the `bible` dataset from https://github.com/jungyeul/korean-parallel-corpora \n",
    "\n",
    "no preprocessing is done except to remove the initial verse information, and then the (en, kr) pairs were written to tsv.\n",
    "\n",
    "every tenth sample, starting from the second (index 1), is used for test set; the rest are used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48ed549",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer...\n",
      "fitting source tokenizer...\n",
      "fitting target tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  control_symbols: [NEW3]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 27975 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW3]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=3438883\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9577% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=61\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999577\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 27975 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 42369 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 27975\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 32891\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 32891 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=15807 obj=9.71156 num_tokens=64401 num_tokens/piece=4.07421\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=12637 obj=7.69518 num_tokens=64503 num_tokens/piece=5.1043\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=9476 obj=7.66936 num_tokens=68742 num_tokens/piece=7.25433\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=9472 obj=7.65159 num_tokens=68764 num_tokens/piece=7.25971\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=7.66805 num_tokens=70220 num_tokens/piece=7.97955\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8799 obj=7.66417 num_tokens=70221 num_tokens/piece=7.98057\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  control_symbols: [NEW3]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 27975 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW3]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=2009822\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1040\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 27975 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 54019 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 27975\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 70384\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 70384 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30496 obj=12.7546 num_tokens=149552 num_tokens/piece=4.90399\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25409 obj=11.4902 num_tokens=149921 num_tokens/piece=5.90031\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19056 obj=11.5578 num_tokens=157275 num_tokens/piece=8.25331\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19054 obj=11.5224 num_tokens=157303 num_tokens/piece=8.25564\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14290 obj=11.7372 num_tokens=167248 num_tokens/piece=11.7038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14290 obj=11.6975 num_tokens=167248 num_tokens/piece=11.7038\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10717 obj=11.9973 num_tokens=177535 num_tokens/piece=16.5657\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10717 obj=11.9497 num_tokens=177535 num_tokens/piece=16.5657\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=12.2074 num_tokens=184423 num_tokens/piece=20.9572\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=12.1707 num_tokens=184553 num_tokens/piece=20.9719\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SimpleTranslationDataset(\"data/translation/train_pairs.tsv\", vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62511c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(src_tokenizer, open(\"data/translation/src_tokenizer.pkl\", \"wb\"))\n",
    "pickle.dump(tgt_tokenizer, open(\"data/translation/tgt_tokenizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf5ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SimpleTranslationDataset(\"data/translation/valid_pairs.tsv\", \n",
    "                                        src_tokenizer=src_tokenizer, \n",
    "                                        tgt_tokenizer=tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a24b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 27975\n",
      "test  samples: 3109\n"
     ]
    }
   ],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"test  samples: {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c98cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9165073",
   "metadata": {},
   "source": [
    "## create model, etc\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- only four attention heads are used\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- a small amount of dropout is added to the QK.T (`attn_dropout`) and to the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT\n",
    "- the pre-layernorm configuration is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6b4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=128,\n",
    "     d_in=512, \n",
    "     d_attn=128, \n",
    "     d_ffnn=1024, \n",
    "     attn_heads=4, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.05, \n",
    "     ffnn_dropout=0.05,\n",
    "     shared_vocab=False,\n",
    "     attn_mask_val=-10e8, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc56da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_vocab_sz': 8000,\n",
       " 'tgt_vocab_sz': 8000,\n",
       " 'enc_layers': 6,\n",
       " 'dec_layers': 6,\n",
       " 'seq_len': 256,\n",
       " 'd_vocab': 128,\n",
       " 'd_in': 512,\n",
       " 'd_attn': 128,\n",
       " 'd_ffnn': 1024,\n",
       " 'attn_heads': 4,\n",
       " 'dropout': 0.1,\n",
       " 'attn_dropout': 0.05,\n",
       " 'ffnn_dropout': 0.05,\n",
       " 'pos_encoding': 'sinusoidal',\n",
       " 'shared_vocab': False,\n",
       " 'attn_mask_val': -1000000000.0,\n",
       " 'attn_q_bias': False,\n",
       " 'attn_kv_bias': False,\n",
       " 'attn_out_bias': False,\n",
       " 'ffnn_activation': 'gelu',\n",
       " 'pre_ln': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55856dd",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "we'll use the `OneCycleLR` to roughly approximate the warmup and annealing by 'warming up' 4000 steps and then decaying for 16000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1537e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(mytransformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.001, amsgrad=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=20000, pct_start=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5579fdf",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b8e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:06:18.48] epoch 000 global step 0100: loss:  855.400\tavg:  885.843\n",
      "[15:06:33.25] epoch 000 global step 0200: loss:  596.050\tavg:  769.033\n",
      "[15:06:48.11] epoch 000 global step 0300: loss:  589.208\tavg:  600.988\n",
      "[15:07:02.96] epoch 000 global step 0400: loss:  483.039\tavg:  514.638\n",
      "[15:07:17.80] epoch 000 global step 0500: loss:  399.909\tavg:  447.875\n",
      "[15:07:32.65] epoch 000 global step 0600: loss:  371.637\tavg:  392.230\n",
      "[15:07:47.53] epoch 000 global step 0700: loss:  283.004\tavg:  353.835\n",
      "[15:08:02.44] epoch 000 global step 0800: loss:  237.866\tavg:  326.004\n",
      "[15:08:17.38] epoch 000 global step 0900: loss:  256.727\tavg:  301.190\n",
      "[15:08:32.35] epoch 000 global step 1000: loss:  273.828\tavg:  290.748\n",
      "[15:08:47.27] epoch 000 global step 1100: loss:  203.040\tavg:  283.346\n",
      "[15:09:02.19] epoch 000 global step 1200: loss:  300.926\tavg:  269.114\n",
      "[15:09:17.12] epoch 000 global step 1300: loss:  274.900\tavg:  263.016\n",
      "[15:09:32.04] epoch 000 global step 1400: loss:  298.570\tavg:  259.900\n",
      "[15:09:47.00] epoch 000 global step 1500: loss:  236.300\tavg:  254.054\n",
      "[15:10:01.94] epoch 000 global step 1600: loss:  232.904\tavg:  252.571\n",
      "[15:10:16.89] epoch 000 global step 1700: loss:  210.583\tavg:  253.816\n",
      "\n",
      "[15:10:25.13] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:10:36.72] epoch 000 eval loss:  233.199\n",
      "\n",
      "\n",
      "[15:10:36.72] checkpoint saved!\n",
      "[15:10:45.94] epoch 001 global step 1800: loss:  274.008\tavg:  249.182\n",
      "[15:11:00.92] epoch 001 global step 1900: loss:  256.068\tavg:  243.836\n",
      "[15:11:15.84] epoch 001 global step 2000: loss:  228.119\tavg:  243.304\n",
      "[15:11:45.85] epoch 001 global step 2200: loss:  203.430\tavg:  236.859\n",
      "[15:12:15.86] epoch 001 global step 2400: loss:  259.414\tavg:  234.023\n",
      "[15:12:45.94] epoch 001 global step 2600: loss:  209.408\tavg:  230.394\n",
      "[15:13:15.83] epoch 001 global step 2800: loss:  246.744\tavg:  224.742\n",
      "[15:13:45.79] epoch 001 global step 3000: loss:  172.266\tavg:  219.727\n",
      "[15:14:15.82] epoch 001 global step 3200: loss:  236.821\tavg:  215.811\n",
      "[15:14:45.76] epoch 001 global step 3400: loss:  209.244\tavg:  213.388\n",
      "\n",
      "[15:15:01.36] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:15:12.89] epoch 001 eval loss:  205.056\n",
      "\n",
      "\n",
      "[15:15:12.89] checkpoint saved!\n",
      "[15:15:29.79] epoch 002 global step 3600: loss:  216.116\tavg:  212.932\n",
      "[15:15:59.68] epoch 002 global step 3800: loss:  233.801\tavg:  207.705\n",
      "[15:16:29.61] epoch 002 global step 4000: loss:  223.026\tavg:  204.183\n",
      "[15:16:59.62] epoch 002 global step 4200: loss:  224.804\tavg:  202.028\n",
      "[15:17:29.45] epoch 002 global step 4400: loss:  197.826\tavg:  201.144\n",
      "[15:17:59.31] epoch 002 global step 4600: loss:  208.375\tavg:  197.073\n",
      "[15:18:29.23] epoch 002 global step 4800: loss:  171.988\tavg:  200.314\n",
      "[15:18:59.12] epoch 002 global step 5000: loss:  159.278\tavg:  196.568\n",
      "[15:19:28.95] epoch 002 global step 5200: loss:  185.857\tavg:  194.304\n",
      "\n",
      "[15:19:36.89] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:19:48.40] epoch 002 eval loss:  187.747\n",
      "\n",
      "\n",
      "[15:19:48.40] checkpoint saved!\n",
      "[15:20:12.87] epoch 003 global step 5400: loss:  201.055\tavg:  192.379\n",
      "[15:20:42.73] epoch 003 global step 5600: loss:  188.163\tavg:  191.244\n",
      "[15:21:12.58] epoch 003 global step 5800: loss:  178.876\tavg:  191.146\n",
      "[15:21:42.42] epoch 003 global step 6000: loss:  184.907\tavg:  187.417\n",
      "[15:22:12.27] epoch 003 global step 6200: loss:  203.344\tavg:  188.157\n",
      "[15:22:42.10] epoch 003 global step 6400: loss:  179.400\tavg:  187.000\n",
      "[15:23:11.94] epoch 003 global step 6600: loss:  161.490\tavg:  184.733\n",
      "[15:23:41.79] epoch 003 global step 6800: loss:  187.059\tavg:  187.522\n",
      "\n",
      "[15:24:11.97] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:24:23.50] epoch 003 eval loss:  178.593\n",
      "\n",
      "\n",
      "[15:24:23.50] checkpoint saved!\n",
      "[15:24:25.73] epoch 004 global step 7000: loss:  170.055\tavg:  187.175\n",
      "[15:24:55.53] epoch 004 global step 7200: loss:  155.328\tavg:  182.858\n",
      "[15:25:25.36] epoch 004 global step 7400: loss:  207.552\tavg:  182.483\n",
      "[15:25:55.18] epoch 004 global step 7600: loss:  185.946\tavg:  181.897\n",
      "[15:26:25.08] epoch 004 global step 7800: loss:  170.926\tavg:  182.648\n",
      "[15:26:54.91] epoch 004 global step 8000: loss:  185.362\tavg:  179.754\n",
      "[15:27:24.73] epoch 004 global step 8200: loss:  180.423\tavg:  179.963\n",
      "[15:27:54.58] epoch 004 global step 8400: loss:  181.306\tavg:  179.876\n",
      "[15:28:24.41] epoch 004 global step 8600: loss:  181.171\tavg:  179.560\n",
      "\n",
      "[15:28:46.96] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:28:58.49] epoch 004 eval loss:  173.059\n",
      "\n",
      "\n",
      "[15:28:58.49] checkpoint saved!\n",
      "[15:29:08.30] epoch 005 global step 8800: loss:  172.429\tavg:  177.714\n",
      "[15:29:38.11] epoch 005 global step 9000: loss:  170.585\tavg:  176.464\n",
      "[15:30:07.92] epoch 005 global step 9200: loss:  174.514\tavg:  178.252\n",
      "[15:30:37.84] epoch 005 global step 9400: loss:  149.142\tavg:  174.932\n",
      "[15:31:07.72] epoch 005 global step 9600: loss:  163.473\tavg:  175.860\n",
      "[15:31:37.56] epoch 005 global step 9800: loss:  155.457\tavg:  175.831\n",
      "[15:32:07.46] epoch 005 global step 10000: loss:  169.501\tavg:  173.621\n",
      "[15:32:37.36] epoch 005 global step 10200: loss:  187.753\tavg:  174.350\n",
      "[15:33:07.23] epoch 005 global step 10400: loss:  166.840\tavg:  172.865\n",
      "\n",
      "[15:33:22.20] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:33:33.74] epoch 005 eval loss:  168.505\n",
      "\n",
      "\n",
      "[15:33:33.74] checkpoint saved!\n",
      "[15:33:51.23] epoch 006 global step 10600: loss:  180.432\tavg:  171.914\n",
      "[15:34:21.16] epoch 006 global step 10800: loss:  169.803\tavg:  173.744\n",
      "[15:34:51.02] epoch 006 global step 11000: loss:  175.237\tavg:  171.910\n",
      "[15:35:20.83] epoch 006 global step 11200: loss:  143.320\tavg:  168.460\n",
      "[15:35:50.71] epoch 006 global step 11400: loss:  146.061\tavg:  170.854\n",
      "[15:36:20.55] epoch 006 global step 11600: loss:  190.681\tavg:  168.510\n",
      "[15:36:50.39] epoch 006 global step 11800: loss:  161.113\tavg:  170.824\n",
      "[15:37:20.20] epoch 006 global step 12000: loss:  167.819\tavg:  170.536\n",
      "[15:37:50.00] epoch 006 global step 12200: loss:  167.414\tavg:  170.340\n",
      "\n",
      "[15:37:57.34] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:38:08.87] epoch 006 eval loss:  165.626\n",
      "\n",
      "\n",
      "[15:38:08.87] checkpoint saved!\n",
      "[15:38:33.93] epoch 007 global step 12400: loss:  170.905\tavg:  166.254\n",
      "[15:39:03.98] epoch 007 global step 12600: loss:  159.244\tavg:  169.695\n",
      "[15:39:33.87] epoch 007 global step 12800: loss:  175.417\tavg:  165.688\n",
      "[15:40:03.73] epoch 007 global step 13000: loss:  208.833\tavg:  167.492\n",
      "[15:40:33.56] epoch 007 global step 13200: loss:  194.835\tavg:  165.960\n",
      "[15:41:03.41] epoch 007 global step 13400: loss:  156.088\tavg:  167.134\n",
      "[15:41:33.23] epoch 007 global step 13600: loss:  150.271\tavg:  167.714\n",
      "[15:42:03.08] epoch 007 global step 13800: loss:  186.349\tavg:  168.065\n",
      "\n",
      "[15:42:32.63] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:42:44.15] epoch 007 eval loss:  162.255\n",
      "\n",
      "\n",
      "[15:42:44.15] checkpoint saved!\n",
      "[15:42:46.87] epoch 008 global step 14000: loss:  194.108\tavg:  162.667\n",
      "[15:43:16.60] epoch 008 global step 14200: loss:  152.825\tavg:  162.680\n",
      "[15:43:46.38] epoch 008 global step 14400: loss:  180.556\tavg:  164.472\n",
      "[15:44:16.21] epoch 008 global step 14600: loss:  172.616\tavg:  164.843\n",
      "[15:44:46.06] epoch 008 global step 14800: loss:  179.300\tavg:  162.964\n",
      "[15:45:15.88] epoch 008 global step 15000: loss:  147.744\tavg:  162.579\n",
      "[15:45:45.73] epoch 008 global step 15200: loss:  166.598\tavg:  165.124\n",
      "[15:46:15.58] epoch 008 global step 15400: loss:  186.272\tavg:  163.941\n",
      "[15:46:45.42] epoch 008 global step 15600: loss:  138.625\tavg:  164.168\n",
      "\n",
      "[15:47:07.39] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:47:18.95] epoch 008 eval loss:  161.094\n",
      "\n",
      "\n",
      "[15:47:18.95] checkpoint saved!\n",
      "[15:47:29.34] epoch 009 global step 15800: loss:  158.638\tavg:  161.972\n",
      "[15:47:59.10] epoch 009 global step 16000: loss:  141.384\tavg:  159.934\n",
      "[15:48:28.87] epoch 009 global step 16200: loss:  193.410\tavg:  162.098\n",
      "[15:48:58.65] epoch 009 global step 16400: loss:  148.191\tavg:  163.518\n",
      "[15:49:28.53] epoch 009 global step 16600: loss:  185.487\tavg:  161.614\n",
      "[15:49:58.41] epoch 009 global step 16800: loss:  160.649\tavg:  161.522\n",
      "[15:50:28.27] epoch 009 global step 17000: loss:  162.731\tavg:  161.180\n",
      "[15:50:58.15] epoch 009 global step 17200: loss:  165.410\tavg:  161.454\n",
      "[15:51:28.04] epoch 009 global step 17400: loss:  161.806\tavg:  163.586\n",
      "\n",
      "[15:51:42.42] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:10<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15:51:53.95] epoch 009 eval loss:  160.184\n",
      "\n",
      "\n",
      "[15:51:53.95] checkpoint saved!\n"
     ]
    }
   ],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        x, y_in, y_true, x_lens, y_lens = batch\n",
    "        x = x.to(\"cuda\")\n",
    "        y_in = y_in.to(\"cuda\")\n",
    "        y_true = y_true.to(\"cuda\")\n",
    "        x_lens = x_lens.to(\"cuda\")\n",
    "        y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "        y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "        loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "\n",
    "        loss.backward() \n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        windowed_losses.append(loss.item())\n",
    "        windowed_losses = windowed_losses[-200:]\n",
    "    \n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step <= 2000 and global_step % 100 == 0:\n",
    "            tme = datetime.datetime.now().isoformat()[11:22]\n",
    "            print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f}\".format(\n",
    "                tme, epoch+1, global_step, loss.item()/BATCH_SIZE, np.mean(windowed_losses)/BATCH_SIZE\n",
    "            ))\n",
    "        elif global_step > 2000 and global_step % 200 == 0:\n",
    "            tme = datetime.datetime.now().isoformat()[11:22]\n",
    "            print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f}\".format(\n",
    "                tme, epoch+1, global_step, loss.item()/BATCH_SIZE, np.mean(windowed_losses)/BATCH_SIZE\n",
    "            ))\n",
    "            \n",
    "    # evaluate\n",
    "    eval_losses = []\n",
    "    time.sleep(1)\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    mytransformer.eval()\n",
    "    \n",
    "    for batch in tqdm.tqdm(test_dataloader):\n",
    "        x, y_in, y_true, x_lens, y_lens = batch\n",
    "        x = x.to(\"cuda\")\n",
    "        y_in = y_in.to(\"cuda\")\n",
    "        y_true = y_true.to(\"cuda\")\n",
    "        x_lens = x_lens.to(\"cuda\")\n",
    "        y_lens = y_lens.to(\"cuda\")\n",
    "        y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "        loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "        eval_losses.append(loss.item())\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)/BATCH_SIZE))\n",
    "    \n",
    "    mytransformer.train()\n",
    "    \n",
    "    # save\n",
    "    torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': mytransformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'windowed_losses': windowed_losses,\n",
    "            'avg_loss': np.mean(windowed_losses)/BATCH_SIZE,\n",
    "            'eval_loss': np.mean(eval_losses)/BATCH_SIZE,\n",
    "            'training_config': mytransformer.config,\n",
    "            'batch_size': BATCH_SIZE\n",
    "            }, \"data/translation/checkpoint.pt\")\n",
    "    \n",
    "    print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49501368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
