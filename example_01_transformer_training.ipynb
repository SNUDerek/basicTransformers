{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec2b45f",
   "metadata": {},
   "source": [
    "# Transformer for Translation Task\n",
    "\n",
    "this notebook demonstrates training a transformer for English to Dutch translation from scratch.\n",
    "\n",
    "the `SimpleTranslationDataset` is a basic in-memory dataset that uses on-the-fly `sentencepiece` tokenization.\n",
    "\n",
    "the `TransformerModel` is our model class, which includes the transformer encoder and decoder layers, as well as the input embedding, positional encoding, and output token prediction layers.\n",
    "\n",
    "details about each component, as well as the training loop, are explained in each section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f398ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb280a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc5692",
   "metadata": {},
   "source": [
    "## training config\n",
    "\n",
    "these values are set roughly based on the original *Attention is All You Need* paper.  \n",
    "vocabulary size is reduced from 37,000 to 16,000 because we are using separate encoder and decoder embedding spaces, and our dataset is smaller (2M pairs vs WMT 20014 en-de's 4.5M pairs).  \n",
    "warmup steps are slightly increased from 4,000 to 5,000, and gradient clipping is applied with max norm of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2529fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN =    256\n",
    "VOCAB_SIZE  =  16000\n",
    "BATCH_SIZE  =     16\n",
    "WARM_STEPS =    5000  # loss increase (warmup) steps\n",
    "COOL_STEPS =  195000  # loss decease steps\n",
    "MAX_STEPS  =  200000  # total training steps\n",
    "EVAL_EVERY =   25000  # run evaluation every n steps\n",
    "EVAL_STEPS =    1000  # only run n steps of eval set\n",
    "INIT_LR     = 0.0001  # staring learning rate pre-warmup\n",
    "MAX_LR      = 0.001   # maximum learning rate after warmup\n",
    "GRAD_CLIP   = 5.0     # gradient norm clip value\n",
    "LOSS_WIN    = 32      # use the last n losses for rolling loss\n",
    "\n",
    "RETRAIN_TOKENIZER = False\n",
    "DEMO_JUST_ONE_ITRE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a0ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only printing one cycle, for demo purposes! please disable for full training!\n"
     ]
    }
   ],
   "source": [
    "if DEMO_JUST_ONE_ITRE:\n",
    "    print(\"only printing one cycle, for demo purposes! please disable for full training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7ed55",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "we are training on europarl english-dutch data, which has approx. 2M pairs: https://www.statmt.org/europarl/\n",
    "\n",
    "train and valid split 90/10 like so:\n",
    "\n",
    "```\n",
    "$ awk 'NR % 10 != 1' europarl-v7.nl-en.en > train.en\n",
    "$ awk 'NR % 10 == 1' europarl-v7.nl-en.en > valid.en\n",
    "$ awk 'NR % 10 != 1' europarl-v7.nl-en.nl > train.nl\n",
    "$ awk 'NR % 10 == 1' europarl-v7.nl-en.nl > valid.nl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa97dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_file = \"data/europarl-en-nl/train.en\"\n",
    "train_target_file = \"data/europarl-en-nl/train.nl\"\n",
    "\n",
    "valid_source_file = \"data/europarl-en-nl/valid.en\"\n",
    "valid_target_file = \"data/europarl-en-nl/valid.nl\"\n",
    "\n",
    "src_tokenizer_path = \"data/europarl-en-nl/src_tokenizer_v{}.pkl\".format(VOCAB_SIZE)\n",
    "tgt_tokenizer_path = \"data/europarl-en-nl/tgt_tokenizer_v{}.pkl\".format(VOCAB_SIZE)\n",
    "checkpoint_file = \"/mnt/data/checkpoints/translation_en_nl/checkpoint_v{}.pt\".format(VOCAB_SIZE)  # saving will insert step count into filename... ¯\\_(ツ)_/¯ \n",
    "\n",
    "# three sentences from the validation set\n",
    "sample_sentences = [\n",
    "    \"The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\",\n",
    "    \"There is, in fact, a risk of a military coup in the future.\",\n",
    "    \"This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525f60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.dirname(checkpoint_file)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fe89bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved tokenizers...\n",
      "CPU times: user 1.42 s, sys: 1.4 s, total: 2.82 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(src_tokenizer_path) and os.path.exists(tgt_tokenizer_path) and not RETRAIN_TOKENIZER:\n",
    "    print(\"loading saved tokenizers...\")\n",
    "    src_tokenizer = pickle.load(open(src_tokenizer_path, \"rb\"))\n",
    "    tgt_tokenizer = pickle.load(open(tgt_tokenizer_path, \"rb\"))\n",
    "    train_dataset = SimpleTranslationDataset(\n",
    "        source_file=train_source_file,\n",
    "        target_file=train_target_file,\n",
    "        src_tokenizer=src_tokenizer, \n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    "    )\n",
    "else:\n",
    "    print(\"training new tokenizers...\")\n",
    "    \n",
    "    # first, create the training dataset with only input, output texts\n",
    "    # this will train new source, target tokenizers (or single tokenizer if share_tokenizer is True)\n",
    "    train_dataset = SimpleTranslationDataset(\n",
    "        source_file=train_source_file,\n",
    "        target_file=train_target_file,\n",
    "        vocab_size=VOCAB_SIZE\n",
    "    )\n",
    "    \n",
    "    # you can then get the tokenizers, and pickle them\n",
    "    src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()\n",
    "    pickle.dump(src_tokenizer, open(src_tokenizer_path, \"wb\"))\n",
    "    pickle.dump(tgt_tokenizer, open(tgt_tokenizer_path, \"wb\"))\n",
    "    \n",
    "    # you may also export the id : token mapping\n",
    "    src_tokenizer.export_vocab(src_tokenizer_path.replace(\".pkl\", \".vocab.txt\"))\n",
    "    src_tokenizer.export_vocab(tgt_tokenizer_path.replace(\".pkl\", \".vocab.txt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f6b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then initialize the validation dataset with the pre-fit tokenizers\n",
    "# this will skip the token-fitting and use the pre-fit tokenizers instead\n",
    "valid_dataset = SimpleTranslationDataset(\n",
    "        source_file=valid_source_file,\n",
    "        target_file=valid_target_file, \n",
    "        src_tokenizer=src_tokenizer, \n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78a2282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 1797997\n",
      "valid samples: 199778\n"
     ]
    }
   ],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"valid samples: {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b20903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "\tsource input : I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "\ttarget input : Ik verklaar de zitting van het Europees Parlement, die op vrijdag 17 december werd onderbroken, te zijn hervat. Ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad.\n",
      "\tsource tokens: ▁I ▁declare ▁resumed ▁the ▁session ▁of ▁the ▁European ▁Parliament ▁adjourned ▁on ▁Friday ▁17 ▁December ▁1999 , ▁and ▁I ▁would ▁like ▁once ▁again ▁to ▁wish ▁you ▁a ▁happy ▁new ▁year ▁in ▁the ▁hope ▁that ▁you ▁enjoyed ▁a ▁pleasant ▁f est ive ▁period .\n",
      "\ttarget tokens: ▁Ik ▁verklaar ▁de ▁zitting ▁van ▁het ▁Europees ▁Parlement , ▁die ▁op ▁vrijdag ▁17 ▁december ▁werd ▁onderbroken , ▁te ▁zijn ▁hervat . ▁Ik ▁wens ▁u ▁allen ▁een ▁gelukkig ▁nieuw jaar ▁en ▁hoop ▁dat ▁u ▁een ▁goede ▁vakantie ▁heeft ▁gehad .\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "index 898998\n",
      "\tsource input : But charging EUR 60 - a third of a month's salary in Belarus - for a visa flies in the face of that policy.\n",
      "\ttarget input : Maar 60 euro - een derde van een maandslaris in Wit-Rusland - berekenen voor een visum, haalt dat beleid regelrecht onderuit.\n",
      "\tsource tokens: ▁But ▁charging ▁EUR ▁60 ▁- ▁a ▁third ▁of ▁a ▁month ' s ▁salary ▁in ▁Belarus ▁- ▁for ▁a ▁visa ▁flies ▁in ▁the ▁face ▁of ▁that ▁policy .\n",
      "\ttarget tokens: ▁Maar ▁60 ▁euro ▁- ▁een ▁derde ▁van ▁een ▁maand sla ris ▁in ▁Wit - Rusland ▁- ▁berekenen ▁voor ▁een ▁visum , ▁haalt ▁dat ▁beleid ▁regelrecht ▁onder uit .\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "index 1797996\n",
      "\tsource input : (The sitting was closed at 10.50 a.m.)\n",
      "\ttarget input : (De vergadering wordt om 10.50 uur gesloten)\n",
      "\tsource tokens: ▁( The ▁sitting ▁was ▁closed ▁at ▁10 . 50 ▁a . m . )\n",
      "\ttarget tokens: ▁( De ▁vergadering ▁wordt ▁om ▁10 . 50 ▁uur ▁gesloten )\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the dataset class has a visual tokenization check of first, middle, and last data (to ensure alignment)\n",
    "train_dataset.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff35249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloaders need a collate_fn to zero-pad the results\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,\n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1,\n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e604faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check dataset, dataloader, epoch size:\n",
      "\tmax train steps      : 200000\n",
      "\tbatch size          : 16\n",
      "\ttrain_dataset    len: 1797997\n",
      "\tvalid_dataset    len: 199778\n",
      "\ttrain_dataloader len: 112375\n",
      "\tvalid_dataloader len: 12487\n"
     ]
    }
   ],
   "source": [
    "print(\"check dataset, dataloader, epoch size:\")\n",
    "print(\"\\tmax train steps      :\", MAX_STEPS)\n",
    "print(\"\\tbatch size          :\", BATCH_SIZE)\n",
    "print(\"\\ttrain_dataset    len:\", len(train_dataset))\n",
    "print(\"\\tvalid_dataset    len:\", len(valid_dataset))\n",
    "print(\"\\ttrain_dataloader len:\", len(train_dataloader))\n",
    "print(\"\\tvalid_dataloader len:\", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daedfb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check data loader output shapes:\n",
      "\t torch.Size([16, 256])\n",
      "\t torch.Size([16, 256])\n",
      "\t torch.Size([16, 256])\n",
      "\t torch.Size([16])\n",
      "\t torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(\"check data loader output shapes:\")\n",
    "data_example = next(iter(train_dataloader))\n",
    "for t in data_example:\n",
    "    print(\"\\t\", t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80cda0c",
   "metadata": {},
   "source": [
    "## create model\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- the original Transformer paper seems to suggest weight tying in section 3.4, but following other implementations, we disable this with (`weight_tying=False`)\n",
    "- multi-head attention q, k, v dimension is not necessarily == d_model / heads, following other implementations\n",
    "- a small amount of dropout is added to the query, key and value attention inputs (`attn_dropout`) and the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT (it supports \"relu\", \"selu\" or \"gelu\")\n",
    "- the pre-layer norm (\"pre-LN Transformer\") configuration is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b081204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=128,\n",
    "     d_model=512, \n",
    "     d_attn=128,\n",
    "     d_ffnn=2048, \n",
    "     attn_heads=8, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.05, \n",
    "     ffnn_dropout=0.05,\n",
    "     pos_encoding=\"sinusoidal\",\n",
    "     shared_vocab=False,\n",
    "     weight_tying=False,\n",
    "     attn_mask_val=-1e08, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()\n",
    "\n",
    "# this initializes parameters with xavier uniform\n",
    "mytransformer.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48aae630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_vocab_sz': 16000,\n",
       " 'tgt_vocab_sz': 16000,\n",
       " 'enc_layers': 6,\n",
       " 'dec_layers': 6,\n",
       " 'seq_len': 256,\n",
       " 'd_vocab': 128,\n",
       " 'd_model': 512,\n",
       " 'd_attn': 128,\n",
       " 'd_ffnn': 2048,\n",
       " 'attn_heads': 8,\n",
       " 'dropout': 0.1,\n",
       " 'attn_dropout': 0.05,\n",
       " 'ffnn_dropout': 0.05,\n",
       " 'pos_encoding': 'sinusoidal',\n",
       " 'shared_vocab': False,\n",
       " 'weight_tying': False,\n",
       " 'attn_mask_val': -100000000.0,\n",
       " 'attn_q_bias': False,\n",
       " 'attn_kv_bias': False,\n",
       " 'attn_out_bias': False,\n",
       " 'ffnn_activation': 'gelu',\n",
       " 'pre_ln': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e2034",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "out of laziness, we'll use the torch default `OneCycleLR` scheduler to roughly approximate the warmup and annealing from *Attention is All You Need*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e546c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(mytransformer.parameters(), lr=INIT_LR, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.0001, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=MAX_LR, \n",
    "                                                total_steps=WARM_STEPS+COOL_STEPS, \n",
    "                                                pct_start=WARM_STEPS/(WARM_STEPS+COOL_STEPS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a204ed",
   "metadata": {},
   "source": [
    "## training loop\n",
    "\n",
    "for easier `tqdm` support for arbitrary number of steps between evaluations, we eschew the usual \"for epoch in epoch, for batch in dataset\" and instead use a while loop and a try-except that will tick up each epoch as we finish it. it's slightly convoluted but it provides a way to view progress for arbitrary step count between evaluations (due to the dataset size, evaluating after every epoch would mean waiting 112,000 steps at a minibatch size of 16.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32643d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [1:53:09<00:00,  3.68it/s, global_step=25000, loss=4.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:30:33.83] epoch 001 global step 25000: loss:    4.149\tavg this cycle:    4.474\n",
      "\n",
      "[14:30:34.83] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:33<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[14:32:10.52] epoch 001 eval loss:    4.068\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: De doelstelling van essentieel belang doelstellingen is de doelstelling van de economische en sociale samenhang tussen de Europese Unie in de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er bestaat namelijk een risico van risicobeoordeling.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dat betekent dat er een partnerschap tussen de autoriteiten en de regeringen van de regeringen van de regeringen van deze middelen worden gefinancierd.\n",
      "\n",
      "\n",
      "[14:32:10.52] checkpoint saved!\n",
      "\n",
      "breaking after one epoch for demo\n"
     ]
    }
   ],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "train_iterator = iter(train_dataloader)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "span_losses = []\n",
    "\n",
    "pbar = tqdm.tqdm(total=EVAL_EVERY)\n",
    "\n",
    "while global_step < MAX_STEPS:\n",
    "\n",
    "    global_step += 1\n",
    "    \n",
    "    try:\n",
    "        batch = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        epoch += 1\n",
    "        train_iterator = iter(train_dataloader)\n",
    "        batch = next(train_iterator)\n",
    "\n",
    "    x, y_in, y_true, x_lens, y_lens = batch\n",
    "    x = x.to(\"cuda\")\n",
    "    y_in = y_in.to(\"cuda\")\n",
    "    y_true = y_true.to(\"cuda\")\n",
    "    x_lens = x_lens.to(\"cuda\")\n",
    "    y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "    _, y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "    loss = criterion(y_pred.transpose(1, 2), y_true)\n",
    "    loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "\n",
    "    loss.backward() \n",
    "    torch.nn.utils.clip_grad_norm_(mytransformer.parameters(), GRAD_CLIP)\n",
    "    optimizer.step()\n",
    "    # don't step the scheduler past its max step\n",
    "    # will fail if steps over max, to add try-except just for idiot-proofing\n",
    "    if global_step < (WARM_STEPS + COOL_STEPS):\n",
    "        try:\n",
    "            scheduler.step()\n",
    "        except:\n",
    "            pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    span_losses.append(loss.item())\n",
    "    windowed_losses.append(loss.item())\n",
    "    windowed_losses = windowed_losses[-LOSS_WIN:]\n",
    "\n",
    "    pbar.set_postfix(loss=\"{:.3f}\".format(np.mean(windowed_losses)), global_step=global_step)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    if global_step % EVAL_EVERY == 0:\n",
    "        \n",
    "        pbar.close()\n",
    "        time.sleep(1)\n",
    "            \n",
    "        # end of epoch loss\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg this cycle: {:>8.3f}\".format(\n",
    "            tme, epoch+1, global_step, loss.item(), np.mean(span_losses)\n",
    "        ))\n",
    "\n",
    "        # evaluate\n",
    "        eval_losses = []\n",
    "        time.sleep(1)\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "        time.sleep(1)\n",
    "\n",
    "        mytransformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in tqdm.tqdm(enumerate(valid_dataloader), total=EVAL_STEPS):\n",
    "                x, y_in, y_true, x_lens, y_lens = batch\n",
    "                x = x.to(\"cuda\")\n",
    "                y_in = y_in.to(\"cuda\")\n",
    "                y_true = y_true.to(\"cuda\")\n",
    "                x_lens = x_lens.to(\"cuda\")\n",
    "                y_lens = y_lens.to(\"cuda\")\n",
    "                _, y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "                loss = criterion(y_pred.transpose(1, 2), y_true)\n",
    "                loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "                eval_losses.append(loss.item())\n",
    "                if idx >= EVAL_STEPS:\n",
    "                    break\n",
    "            time.sleep(1)\n",
    "\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)))\n",
    "\n",
    "        # infer some results\n",
    "        time.sleep(1)\n",
    "        print(\"\\n sample greedy outputs:\\n\")\n",
    "        with torch.no_grad():\n",
    "            for sample in sample_sentences:\n",
    "                x, x_len = src_tokenizer.transform(sample, as_array=True, bos=True, eos=True, max_len=MAX_SEQ_LEN)\n",
    "                x = torch.from_numpy(x).long().to(\"cuda\")\n",
    "                x_len = torch.from_numpy(x_len).long().to(\"cuda\")\n",
    "                y_hat = mytransformer.infer_one_greedy(x, x_len, bos=2, eos=3)\n",
    "                y_hat = tgt_tokenizer.inverse_transform([y_hat], as_tokens=False)[0]\n",
    "                print(\"\\tsrc: {}\".format(sample))\n",
    "                print(\"\\tprd: {}\\n\".format(y_hat))\n",
    "\n",
    "        # save\n",
    "        torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': mytransformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'windowed_losses': windowed_losses,\n",
    "                'avg_loss': np.mean(windowed_losses),\n",
    "                'eval_loss': np.mean(eval_losses),\n",
    "                'training_config': mytransformer.config,\n",
    "                'batch_size': BATCH_SIZE\n",
    "                }, checkpoint_file.replace(\".pt\", \"-{:08d}.pt\".format(global_step)))\n",
    "\n",
    "        print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "\n",
    "        mytransformer.train()\n",
    "        \n",
    "        span_losses = []\n",
    "        \n",
    "        if DEMO_JUST_ONE_ITRE:\n",
    "            print(\"\\nbreaking after one epoch for demo\")\n",
    "            break\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=EVAL_EVERY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
