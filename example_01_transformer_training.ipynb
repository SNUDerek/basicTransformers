{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128d8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222e4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd37859",
   "metadata": {},
   "source": [
    "## training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb9e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 8000\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 64\n",
    "GRAD_CLIP = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2979b03",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "this is being trained for English > Korean translation on the `korean-english-news-v1` dataset from https://github.com/jungyeul/korean-parallel-corpora \n",
    "\n",
    "no preprocessing is done except to read in the data and write the (en, kr) pairs to tsv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b27cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/translation_news/en-ko-train.tsv\"\n",
    "valid_file = \"data/translation_news/en-ko-dev.tsv\"\n",
    "train_tokenizer = \"data/translation_news/src_tokenizer.pkl\"\n",
    "valid_tokenizer = \"data/translation_news/tgt_tokenizer.pkl\"\n",
    "checkpoint_file = \"data/translation_news/checkpoint.pt\"\n",
    "sample_sentences = [\n",
    "    \"After keeping the world's most powerful supercomputer to themselves for a year, government researchers showed off the $110 million wonder and said it might help save the world from nuclear war.\",\n",
    "    \"Most of the people involved in the discussion agree that there is a legitimate area in which the government needs to retain the right to intercept communications.\",\n",
    "    \"Several Texan transmission companies announced Monday they were forming a consortium to invest in the $5 billion cost of building new power lines to take advantage of the state's vast wind power.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa7eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for training with the alternate 'bible' dataset.\n",
    "# # here, the verse notes are removed, and every tenth item starting from the second (index = 1) is set to validation\n",
    "# train_file = \"data/translation_bible/train_pairs.tsv\"\n",
    "# valid_file = \"data/translation_bible/valid_pairs.tsv\"\n",
    "# train_tokenizer = \"data/translation_bible/src_tokenizer.pkl\"\n",
    "# valid_tokenizer = \"data/translation_bible/tgt_tokenizer.pkl\"\n",
    "# checkpoint_file = \"data/translation_bible/checkpoint.pt\"\n",
    "# sample_sentences = [\n",
    "#     \"The weapons we fight with are not the weapons of the world. On the contrary, they have divine power to demolish strongholds.\",\n",
    "#     \"Make it your ambition to lead a quiet life, to mind your own business and to work with your hands, just as we told you,\",\n",
    "#     \"It had a great, high wall with twelve gates, and with twelve angels at the gates. On the gates were written the names of the twelve tribes of Israel.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdff59e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer...\n",
      "fitting source tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  control_symbols: [NEW3]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting target tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW3]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=11967043\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.955% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=82\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.99955\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 142416 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 109507\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 109507 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=55808 obj=11.4654 num_tokens=233550 num_tokens/piece=4.18488\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=47027 obj=9.15743 num_tokens=234355 num_tokens/piece=4.98341\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35267 obj=9.14063 num_tokens=247365 num_tokens/piece=7.01406\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35257 obj=9.12308 num_tokens=247528 num_tokens/piece=7.02068\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26442 obj=9.21581 num_tokens=267778 num_tokens/piece=10.127\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26442 obj=9.19271 num_tokens=267762 num_tokens/piece=10.1264\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19831 obj=9.33007 num_tokens=291501 num_tokens/piece=14.6993\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19831 obj=9.30291 num_tokens=291468 num_tokens/piece=14.6976\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14873 obj=9.48464 num_tokens=315871 num_tokens/piece=21.2379\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14873 obj=9.4509 num_tokens=315857 num_tokens/piece=21.2369\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11154 obj=9.679 num_tokens=340152 num_tokens/piece=30.496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11154 obj=9.63773 num_tokens=340115 num_tokens/piece=30.4926\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=9.85836 num_tokens=359983 num_tokens/piece=40.9072\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=9.82062 num_tokens=360102 num_tokens/piece=40.9207\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  control_symbols: [NEW3]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW3]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5811421\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1324\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 176807 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241207\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241207 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=95151 obj=14.8542 num_tokens=531109 num_tokens/piece=5.58175\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=83596 obj=13.516 num_tokens=533507 num_tokens/piece=6.38197\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62692 obj=13.5434 num_tokens=554730 num_tokens/piece=8.8485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62663 obj=13.5039 num_tokens=555221 num_tokens/piece=8.86043\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46996 obj=13.6773 num_tokens=583777 num_tokens/piece=12.4218\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46995 obj=13.6361 num_tokens=584070 num_tokens/piece=12.4283\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35246 obj=13.867 num_tokens=615085 num_tokens/piece=17.4512\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35246 obj=13.8185 num_tokens=615078 num_tokens/piece=17.451\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26434 obj=14.1044 num_tokens=647378 num_tokens/piece=24.4904\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26434 obj=14.05 num_tokens=647384 num_tokens/piece=24.4906\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19825 obj=14.3784 num_tokens=680954 num_tokens/piece=34.3482\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19825 obj=14.3167 num_tokens=681044 num_tokens/piece=34.3528\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14868 obj=14.6847 num_tokens=716521 num_tokens/piece=48.1922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14868 obj=14.6141 num_tokens=716522 num_tokens/piece=48.1922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11151 obj=15.0565 num_tokens=752674 num_tokens/piece=67.4983\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11151 obj=14.9758 num_tokens=752672 num_tokens/piece=67.4982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3823 num_tokens=782772 num_tokens/piece=88.9514\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.3086 num_tokens=782842 num_tokens/piece=88.9593\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SimpleTranslationDataset(train_file, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448f65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d229ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(src_tokenizer, open(train_tokenizer, \"wb\"))\n",
    "pickle.dump(tgt_tokenizer, open(valid_tokenizer, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99817638",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SimpleTranslationDataset(valid_file, \n",
    "                                         src_tokenizer=src_tokenizer, \n",
    "                                         tgt_tokenizer=tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed29e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 94123\n",
      "valid samples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"valid samples: {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b8a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fdda1",
   "metadata": {},
   "source": [
    "## create model, etc\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- a small amount of dropout is added to the QK.T (`attn_dropout`) and to the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT\n",
    "- the pre-layernorm configuration is used\n",
    "- AdamW optimizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5aca225",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=256,\n",
    "     d_in=512, \n",
    "     d_attn=64, \n",
    "     d_ffnn=1024, \n",
    "     attn_heads=8, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.05, \n",
    "     ffnn_dropout=0.05,\n",
    "     pos_encoding=\"sinusoidal\",\n",
    "     shared_vocab=False,\n",
    "     attn_mask_val=-1e08, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c348af89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_vocab_sz': 8000,\n",
       " 'tgt_vocab_sz': 8000,\n",
       " 'enc_layers': 6,\n",
       " 'dec_layers': 6,\n",
       " 'seq_len': 256,\n",
       " 'd_vocab': 256,\n",
       " 'd_in': 512,\n",
       " 'd_attn': 64,\n",
       " 'd_ffnn': 1024,\n",
       " 'attn_heads': 8,\n",
       " 'dropout': 0.1,\n",
       " 'attn_dropout': 0.05,\n",
       " 'ffnn_dropout': 0.05,\n",
       " 'pos_encoding': 'sinusoidal',\n",
       " 'shared_vocab': False,\n",
       " 'attn_mask_val': -100000000.0,\n",
       " 'attn_q_bias': False,\n",
       " 'attn_kv_bias': False,\n",
       " 'attn_out_bias': False,\n",
       " 'ffnn_activation': 'gelu',\n",
       " 'pre_ln': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8a114",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "we'll use the `OneCycleLR` to roughly approximate the warmup and annealing by 'warming up' for 1 epoch and then decaying for 49 epochs (until 50th epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45e4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(mytransformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.0001, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=len(train_dataloader)*50, pct_start=1./50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5cf49",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaa09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 of 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch:   3%|‚ñà‚ñà‚ñà‚ñä                                                                                                                               | 174/5883 [00:32<18:10,  5.23it/s, global_step=174, loss=545.209]"
     ]
    }
   ],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    print(\"starting epoch {} of {}\".format(epoch+1, MAX_EPOCHS))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader, desc=\"minibatch\", total=len(train_dataloader)) as b:\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(mytransformer.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            windowed_losses.append(loss.item())\n",
    "            windowed_losses = windowed_losses[-16:]\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            b.set_postfix(loss=\"{:.3f}\".format(np.mean(windowed_losses)), global_step=global_step)\n",
    "            b.update(1)\n",
    "            \n",
    "    # end of epoch loss\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f} (end of epoch)\".format(\n",
    "        tme, epoch+1, global_step, loss.item(), np.mean(windowed_losses)\n",
    "    ))\n",
    "        \n",
    "    # evaluate\n",
    "    eval_losses = []\n",
    "    time.sleep(1)\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    mytransformer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(valid_dataloader):\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "            eval_losses.append(loss.item())\n",
    "            \n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # infer some results\n",
    "    print(\"\\n sample greedy outputs:\\n\")\n",
    "    with torch.no_grad():\n",
    "        for sample in sample_sentences:\n",
    "            x, x_len = src_tokenizer.transform(sample, as_array=True, bos=True, eos=True, max_len=MAX_SEQ_LEN)\n",
    "            x = torch.from_numpy(x).long().to(\"cuda\")\n",
    "            x_len = torch.from_numpy(x_len).long().to(\"cuda\")\n",
    "            y_hat = mytransformer.infer_one_greedy(x, x_len, bos=2, eos=3)\n",
    "            y_hat = tgt_tokenizer.inverse_transform([y_hat], as_tokens=False)[0]\n",
    "            print(\"\\tsrc: {}\".format(sample))\n",
    "            print(\"\\tprd: {}\\n\".format(y_hat))\n",
    "    \n",
    "    \n",
    "    # save\n",
    "    torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': mytransformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'windowed_losses': windowed_losses,\n",
    "            'avg_loss': np.mean(windowed_losses),\n",
    "            'eval_loss': np.mean(eval_losses),\n",
    "            'training_config': mytransformer.config,\n",
    "            'batch_size': BATCH_SIZE\n",
    "            }, checkpoint_file)\n",
    "    \n",
    "    print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "    \n",
    "    \n",
    "    mytransformer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "-1e08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c5ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
