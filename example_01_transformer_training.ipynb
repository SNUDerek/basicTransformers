{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa31f05",
   "metadata": {},
   "source": [
    "# Transformer for Translation Task\n",
    "\n",
    "this notebook demonstrates training a transformer for English to Dutch translation from scratch.\n",
    "\n",
    "the `SimpleTranslationDataset` is a basic in-memory dataset that uses on-the-fly `sentencepiece` tokenization.\n",
    "\n",
    "the `TransformerModel` is our model class, which includes the transformer encoder and decoder layers, as well as the input embedding, positional encoding, and output token prediction layers.\n",
    "\n",
    "details about each component, as well as the training loop, are explained in each section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5e4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa444eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2a42f",
   "metadata": {},
   "source": [
    "## training config\n",
    "\n",
    "these values are set roughly based on the original *Attention is All You Need* paper.  \n",
    "vocabulary size is reduced from 37,000 to 16,000 because we are using separate encoder and decoder embedding spaces, and our dataset is smaller (2M pairs vs WMT 20014 en-de's 4.5M pairs).  \n",
    "warmup steps are slightly increased from 4,000 to 5,000, and gradient clipping is applied with max norm of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af97d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN =    256\n",
    "VOCAB_SIZE  =  16000\n",
    "BATCH_SIZE  =     16\n",
    "WARM_STEPS =    5000  # loss increase (warmup) steps\n",
    "COOL_STEPS =  495000  # loss decease steps\n",
    "MAX_STEPS  = 1000000  # total training steps\n",
    "EVAL_EVERY =   20000  # run evaluation every n steps\n",
    "EVAL_STEPS =     500  # only run n steps of eval set\n",
    "INIT_LR     = 0.0001  # staring learning rate pre-warmup\n",
    "MAX_LR      = 0.001   # maximum learning rate after warmup\n",
    "GRAD_CLIP   = 5.0     # gradient norm clip value\n",
    "LOSS_WIN    = 32      # use the last n losses for rolling loss\n",
    "RETRAIN_TOKENIZER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe521365",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "we are training on europarl english-dutch data, which has approx. 2M pairs: https://www.statmt.org/europarl/\n",
    "\n",
    "train and valid split 90/10 like so:\n",
    "\n",
    "```\n",
    "$ awk 'NR % 10 != 1' europarl-v7.nl-en.en > train.en\n",
    "$ awk 'NR % 10 == 1' europarl-v7.nl-en.en > valid.en\n",
    "$ awk 'NR % 10 != 1' europarl-v7.nl-en.nl > train.nl\n",
    "$ awk 'NR % 10 == 1' europarl-v7.nl-en.nl > valid.nl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51385fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_file = \"data/europarl-en-nl/train.en\"\n",
    "train_target_file = \"data/europarl-en-nl/train.nl\"\n",
    "\n",
    "valid_source_file = \"data/europarl-en-nl/valid.en\"\n",
    "valid_target_file = \"data/europarl-en-nl/valid.nl\"\n",
    "\n",
    "src_tokenizer_path = \"data/europarl-en-nl/src_tokenizer_v{}.pkl\".format(VOCAB_SIZE)\n",
    "tgt_tokenizer_path = \"data/europarl-en-nl/tgt_tokenizer_v{}.pkl\".format(VOCAB_SIZE)\n",
    "checkpoint_file = \"data/translation_news/checkpoint_v{}.pt\".format(VOCAB_SIZE)\n",
    "\n",
    "# three sentences from the validation set\n",
    "sample_sentences = [\n",
    "    \"The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\",\n",
    "    \"There is, in fact, a risk of a military coup in the future.\",\n",
    "    \"This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a43758",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved tokenizers...\n",
      "CPU times: user 1.42 s, sys: 1.28 s, total: 2.7 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(src_tokenizer_path) and os.path.exists(tgt_tokenizer_path) and not RETRAIN_TOKENIZER:\n",
    "    print(\"loading saved tokenizers...\")\n",
    "    src_tokenizer = pickle.load(open(src_tokenizer_path, \"rb\"))\n",
    "    tgt_tokenizer = pickle.load(open(tgt_tokenizer_path, \"rb\"))\n",
    "    train_dataset = SimpleTranslationDataset(\n",
    "        source_file=train_source_file,\n",
    "        target_file=train_target_file,\n",
    "        src_tokenizer=src_tokenizer, \n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    "    )\n",
    "else:\n",
    "    print(\"training new tokenizers...\")\n",
    "    \n",
    "    # first, create the training dataset with only input, output texts\n",
    "    # this will train new source, target tokenizers (or single tokenizer if share_tokenizer is True)\n",
    "    train_dataset = SimpleTranslationDataset(\n",
    "        source_file=train_source_file,\n",
    "        target_file=train_target_file,\n",
    "        vocab_size=VOCAB_SIZE\n",
    "    )\n",
    "    \n",
    "    # you can then get the tokenizers, and pickle them\n",
    "    src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()\n",
    "    pickle.dump(src_tokenizer, open(src_tokenizer_path, \"wb\"))\n",
    "    pickle.dump(tgt_tokenizer, open(tgt_tokenizer_path, \"wb\"))\n",
    "    \n",
    "    # you may also export the id : token mapping\n",
    "    src_tokenizer.export_vocab(src_tokenizer_path.replace(\".pkl\", \".vocab.txt\"))\n",
    "    src_tokenizer.export_vocab(tgt_tokenizer_path.replace(\".pkl\", \".vocab.txt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63945f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then initialize the validation dataset with the pre-fit tokenizers\n",
    "# this will skip the token-fitting and use the pre-fit tokenizers instead\n",
    "valid_dataset = SimpleTranslationDataset(\n",
    "        source_file=valid_source_file,\n",
    "        target_file=valid_target_file, \n",
    "        src_tokenizer=src_tokenizer, \n",
    "        tgt_tokenizer=tgt_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fab3a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 1797997\n",
      "valid samples: 199778\n"
     ]
    }
   ],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"valid samples: {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cca3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "\tsource input : I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "\ttarget input : Ik verklaar de zitting van het Europees Parlement, die op vrijdag 17 december werd onderbroken, te zijn hervat. Ik wens u allen een gelukkig nieuwjaar en hoop dat u een goede vakantie heeft gehad.\n",
      "\tsource tokens: ▁I ▁declare ▁resumed ▁the ▁session ▁of ▁the ▁European ▁Parliament ▁adjourned ▁on ▁Friday ▁17 ▁December ▁1999 , ▁and ▁I ▁would ▁like ▁once ▁again ▁to ▁wish ▁you ▁a ▁happy ▁new ▁year ▁in ▁the ▁hope ▁that ▁you ▁enjoyed ▁a ▁pleasant ▁f est ive ▁period .\n",
      "\ttarget tokens: ▁Ik ▁verklaar ▁de ▁zitting ▁van ▁het ▁Europees ▁Parlement , ▁die ▁op ▁vrijdag ▁17 ▁december ▁werd ▁onderbroken , ▁te ▁zijn ▁hervat . ▁Ik ▁wens ▁u ▁allen ▁een ▁gelukkig ▁nieuw jaar ▁en ▁hoop ▁dat ▁u ▁een ▁goede ▁vakantie ▁heeft ▁gehad .\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "index 898998\n",
      "\tsource input : But charging EUR 60 - a third of a month's salary in Belarus - for a visa flies in the face of that policy.\n",
      "\ttarget input : Maar 60 euro - een derde van een maandslaris in Wit-Rusland - berekenen voor een visum, haalt dat beleid regelrecht onderuit.\n",
      "\tsource tokens: ▁But ▁charging ▁EUR ▁60 ▁- ▁a ▁third ▁of ▁a ▁month ' s ▁salary ▁in ▁Belarus ▁- ▁for ▁a ▁visa ▁flies ▁in ▁the ▁face ▁of ▁that ▁policy .\n",
      "\ttarget tokens: ▁Maar ▁60 ▁euro ▁- ▁een ▁derde ▁van ▁een ▁maand sla ris ▁in ▁Wit - Rusland ▁- ▁berekenen ▁voor ▁een ▁visum , ▁haalt ▁dat ▁beleid ▁regelrecht ▁onder uit .\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "index 1797996\n",
      "\tsource input : (The sitting was closed at 10.50 a.m.)\n",
      "\ttarget input : (De vergadering wordt om 10.50 uur gesloten)\n",
      "\tsource tokens: ▁( The ▁sitting ▁was ▁closed ▁at ▁10 . 50 ▁a . m . )\n",
      "\ttarget tokens: ▁( De ▁vergadering ▁wordt ▁om ▁10 . 50 ▁uur ▁gesloten )\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the dataset class has a visual tokenization check of first, middle, and last data (to ensure alignment)\n",
    "train_dataset.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd31fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloaders need a collate_fn to zero-pad the results\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be01b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "112375\n",
      "12487\n",
      "8.898776418242491\n"
     ]
    }
   ],
   "source": [
    "print(MAX_STEPS)\n",
    "print(len(train_dataloader))\n",
    "print(len(valid_dataloader))\n",
    "print(MAX_STEPS/len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959547f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18cfd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256])\n",
      "torch.Size([16, 256])\n",
      "torch.Size([16, 256])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for t in data_example:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455287d",
   "metadata": {},
   "source": [
    "## create model\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- the original Transformer paper seems to suggest weight tying in section 3.4, but following other implementations, we disable this with (`weight_tying=False`)\n",
    "- multi-head attention q, k, v dimension is not necessarily == d_model / heads, following other implementations\n",
    "- a small amount of dropout is added to the query, key and value attention inputs (`attn_dropout`) and the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT (it supports \"relu\", \"selu\" or \"gelu\")\n",
    "- the pre-layer norm (\"pre-LN Transformer\") configuration is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24dca283",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=128,\n",
    "     d_model=512, \n",
    "     d_attn=128,\n",
    "     d_ffnn=2048, \n",
    "     attn_heads=8, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.05, \n",
    "     ffnn_dropout=0.05,\n",
    "     pos_encoding=\"sinusoidal\",\n",
    "     shared_vocab=False,\n",
    "     weight_tying=False,\n",
    "     attn_mask_val=-1e08, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()\n",
    "\n",
    "# this initializes parameters with xavier uniform\n",
    "mytransformer.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339099ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_vocab_sz': 16000,\n",
       " 'tgt_vocab_sz': 16000,\n",
       " 'enc_layers': 6,\n",
       " 'dec_layers': 6,\n",
       " 'seq_len': 256,\n",
       " 'd_vocab': 128,\n",
       " 'd_model': 512,\n",
       " 'd_attn': 128,\n",
       " 'd_ffnn': 2048,\n",
       " 'attn_heads': 8,\n",
       " 'dropout': 0.1,\n",
       " 'attn_dropout': 0.05,\n",
       " 'ffnn_dropout': 0.05,\n",
       " 'pos_encoding': 'sinusoidal',\n",
       " 'shared_vocab': False,\n",
       " 'weight_tying': False,\n",
       " 'attn_mask_val': -100000000.0,\n",
       " 'attn_q_bias': False,\n",
       " 'attn_kv_bias': False,\n",
       " 'attn_out_bias': False,\n",
       " 'ffnn_activation': 'gelu',\n",
       " 'pre_ln': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b04985",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "out of laziness, we'll use the torch default `OneCycleLR` scheduler to roughly approximate the warmup and annealing from *Attention is All You Need*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e095f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(mytransformer.parameters(), lr=INIT_LR, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.0001, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=MAX_LR, \n",
    "                                                total_steps=WARM_STEPS+COOL_STEPS, \n",
    "                                                pct_start=WARM_STEPS/(WARM_STEPS+COOL_STEPS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aeca45",
   "metadata": {},
   "source": [
    "## training loop\n",
    "\n",
    "for easier `tqdm` support for arbitrary number of steps between evaluations, we eschew the usual \"for epoch in epoch, for batch in dataset\" and instead use a while loop and a try-except that will tick up each epoch as we finish it. it's slightly convoluted but it provides a way to view progress for arbitrary step count between evaluations (due to the dataset size, evaluating after every epoch would mean waiting 112,000 steps at a minibatch size of 16.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:06<00:00,  3.74it/s, global_step=2e+4, loss=4.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:57:36.73] epoch 024 global step 20000: loss:    4.382\tavg:    4.296\n",
      "\n",
      "[01:57:37.74] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[01:58:26.91] epoch 024 eval loss:    4.068\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het belangrijkste doel is de structuurfondsen te versterken om de economische cohesie en economische cohesie tussen de Europese Unie te versterken.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is een risico voor een toekomst van een militaire toekomst.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dit betekent dat er een akkoord tussen een interinstitutionele en regionale autoriteiten moeten worden ontwikkeld om de lidstaten te maken.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[01:58:26.91] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:04<00:00,  3.74it/s, global_step=4e+4, loss=4.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:27:36.89] epoch 055 global step 40000: loss:    4.657\tavg:    4.223\n",
      "\n",
      "[03:27:37.89] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[03:28:27.05] epoch 055 eval loss:    4.012\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het doel van destructuurfondsen is de economische en sociale samenhang tussen de Europese Unie en de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is een risico, een gevaar voor een toekomst van de toekomst.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dat betekent dat er een goed partnerschap tussen regeringen en nationale regeringen moeten worden besteed aan de nationale regeringen van deze middelen.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[03:28:27.05] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:10<00:00,  3.74it/s, global_step=6e+4, loss=4.077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:57:43.00] epoch 076 global step 60000: loss:    4.556\tavg:    4.077\n",
      "\n",
      "[04:57:44.00] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[04:58:33.14] epoch 076 eval loss:    3.995\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het doel van de structuurfondsen is de versterking van de versterking van de economische en cohesie tussen de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is een risico, een risico voor een risico in de toekomst.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dit betekent dat er een evenwichtige samenwerking tussen de nationale overheden en de nationale overheden worden uitgevoerd.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[04:58:33.14] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:10<00:00,  3.74it/s, global_step=8e+4, loss=4.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:27:49.10] epoch 096 global step 80000: loss:    3.985\tavg:    4.124\n",
      "\n",
      "[06:27:50.10] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[06:28:39.34] epoch 096 eval loss:    3.959\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het belangrijkste doel van de structurele structuurfondsen is een belangrijk doel tussen de sociale en sociale cohesie tussen de Europese Unie en de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is echter een risicobeoordeling van een militaire risicobeoordeling.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Het betekent dat er een open en ander partnerschap tussen de nationale autoriteiten en de nationale regeringen moet worden aangepakt.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[06:28:39.34] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:11<00:00,  3.74it/s, global_step=1e+5, loss=4.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:57:56.58] epoch 117 global step 100000: loss:    4.202\tavg:    4.071\n",
      "\n",
      "[07:57:57.59] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[07:58:46.72] epoch 117 eval loss:    3.948\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het is van cruciaal belang dat de structuurfondsen middelen en de economische samenhang tussen de economische en sociale samenhang tussen de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is een risico dat een risico is een risico op militaire manier in de toekomst.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dat betekent dat er een partnerschap tussen de plaatselijke overheden en de nationale overheden moeten worden besteed aan de financiële middelen.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[07:58:46.72] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [1:29:16<00:00,  3.73it/s, global_step=120000, loss=3.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:28:09.03] epoch 137 global step 120000: loss:    3.948\tavg:    3.992\n",
      "\n",
      "[09:28:10.03] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:47, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[09:28:59.28] epoch 137 eval loss:    3.906\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: The key goal of the structural funds is to strengthen social and economic cohesion between the regions within the European Union.\n",
      "\tprd: Het belangrijkste doel van de structuurfondsen is een betere samenhang tussen de sociale samenhang en de sociale samenhang binnen de Europese Unie.\n",
      "\n",
      "\tsrc: There is, in fact, a risk of a military coup in the future.\n",
      "\tprd: Er is een risico op een militaire militaire militaire militaire macht.\n",
      "\n",
      "\tsrc: This means that there must be a comprehensive partnership between local authorities and national governments with regard to how these funds are to be spent.\n",
      "\tprd: Dit betekent dat er een associatie tussen de nationale autoriteiten en nationale autoriteiten worden betrokken.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[09:28:59.28] checkpoint saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████████████████▎                                                                                                            | 4135/20000 [18:27<1:11:06,  3.72it/s, global_step=124135, loss=3.993]"
     ]
    }
   ],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "train_iterator = iter(train_dataloader)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "pbar = tqdm.tqdm(total=EVAL_EVERY)\n",
    "\n",
    "while global_step < MAX_STEPS:\n",
    "\n",
    "    try:\n",
    "        batch = next(train_iterator)\n",
    "    except:\n",
    "        epoch += 1\n",
    "        train_iterator = iter(train_dataloader)\n",
    "        batch = next(train_iterator)\n",
    "\n",
    "    x, y_in, y_true, x_lens, y_lens = batch\n",
    "    x = x.to(\"cuda\")\n",
    "    y_in = y_in.to(\"cuda\")\n",
    "    y_true = y_true.to(\"cuda\")\n",
    "    x_lens = x_lens.to(\"cuda\")\n",
    "    y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "    _, y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "    loss = criterion(y_pred.transpose(1, 2), y_true)\n",
    "    loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "\n",
    "    loss.backward() \n",
    "    # torch.nn.utils.clip_grad_norm_(mytransformer.parameters(), GRAD_CLIP)\n",
    "    optimizer.step()\n",
    "    # don't step the scheduler past its max step\n",
    "    if global_step <= (WARM_STEPS + COOL_STEPS):\n",
    "        scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    windowed_losses.append(loss.item())\n",
    "    windowed_losses = windowed_losses[-LOSS_WIN:]\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "    pbar.set_postfix(loss=\"{:.3f}\".format(np.mean(windowed_losses)), global_step=global_step)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    if global_step % EVAL_EVERY == 0:\n",
    "        \n",
    "        pbar.close()\n",
    "        time.sleep(1)\n",
    "            \n",
    "        # end of epoch loss\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f}\".format(\n",
    "            tme, epoch+1, global_step, loss.item(), np.mean(windowed_losses)\n",
    "        ))\n",
    "\n",
    "        # evaluate\n",
    "        eval_losses = []\n",
    "        time.sleep(1)\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "        time.sleep(1)\n",
    "\n",
    "        mytransformer.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in tqdm.tqdm(enumerate(valid_dataloader)):\n",
    "                x, y_in, y_true, x_lens, y_lens = batch\n",
    "                x = x.to(\"cuda\")\n",
    "                y_in = y_in.to(\"cuda\")\n",
    "                y_true = y_true.to(\"cuda\")\n",
    "                x_lens = x_lens.to(\"cuda\")\n",
    "                y_lens = y_lens.to(\"cuda\")\n",
    "                _, y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "                loss = criterion(y_pred.transpose(1, 2), y_true)\n",
    "                loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "                eval_losses.append(loss.item())\n",
    "                if idx >= EVAL_STEPS:\n",
    "                    break\n",
    "            time.sleep(1)\n",
    "\n",
    "        tme = datetime.datetime.now().isoformat()[11:22]\n",
    "        print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)))\n",
    "\n",
    "        # infer some results\n",
    "        time.sleep(1)\n",
    "        print(\"\\n sample greedy outputs:\\n\")\n",
    "        with torch.no_grad():\n",
    "            for sample in sample_sentences:\n",
    "                x, x_len = src_tokenizer.transform(sample, as_array=True, bos=True, eos=True, max_len=MAX_SEQ_LEN)\n",
    "                x = torch.from_numpy(x).long().to(\"cuda\")\n",
    "                x_len = torch.from_numpy(x_len).long().to(\"cuda\")\n",
    "                y_hat = mytransformer.infer_one_greedy(x, x_len, bos=2, eos=3)\n",
    "                y_hat = tgt_tokenizer.inverse_transform([y_hat], as_tokens=False)[0]\n",
    "                print(\"\\tsrc: {}\".format(sample))\n",
    "                print(\"\\tprd: {}\\n\".format(y_hat))\n",
    "\n",
    "        # save\n",
    "        torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': mytransformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'windowed_losses': windowed_losses,\n",
    "                'avg_loss': np.mean(windowed_losses),\n",
    "                'eval_loss': np.mean(eval_losses),\n",
    "                'training_config': mytransformer.config,\n",
    "                'batch_size': BATCH_SIZE\n",
    "                }, checkpoint_file)\n",
    "\n",
    "        print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "\n",
    "        mytransformer.train()\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=EVAL_EVERY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4782ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with masks: stuck at ~4.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
